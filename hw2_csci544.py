# -*- coding: utf-8 -*-
# """HW2_CSCI544.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1448Y5dOMnhfqD6R12BVtFA4wFPVrJVor


# from google.colab import drive
# drive.mount('/content/drive')

# """Importing needed libraries"""

import json
import numpy as np
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# """Loading Required data"""

#Loading the train data
with open('/content/drive/MyDrive/data/train.json') as f:
  train_data = json.load(f)

#Loading the development data
with open('/content/drive/MyDrive/data/dev.json') as fi:
  dev_data = json.load(fi)

#Loading the test data
with open('/content/drive/MyDrive/data/test.json') as fil:
  test_data = json.load(fil)

# """Declaring necessary function"""

def create_json(data, pred_list):
    new_file = []
    for c in data:
        x = c
        x['labels'] = pred_list[c['index']]
        new_file.append(x)

    return new_file

def hmm_accuracy(og, test):
  count = 0
  for i in range(0, len(test)):
    if(test[i] == og[i]):
      count += 1

  # print(count / 37)
  return count/len(og)

# """Task 1: Vocabulary Creation"""

word_freq = {}
for item in train_data:
  for word in item['sentence']:
    word = word.lower()
    if(word in word_freq):
      word_freq[word] += 1
    else:
      word_freq[word] = 1

label_freq = {}
for item in train_data:
    for label in item['labels']:
        if label in label_freq:
            label_freq[label] += 1
        else:
            label_freq[label] = 1

dev_labels = np.array([np.array(x['labels']) for x in dev_data])
dev_labels = np.concatenate(dev_labels)

stopping_threshold = 2
unknown_counter = 0
for k in list(word_freq):
  if word_freq[k] < stopping_threshold:
    unknown_counter = unknown_counter + word_freq[k]
    del word_freq[k]

# word_freq['<unk>'] = unknown_counter

# """I used 2 as my threshold"""

print("Total number of words replaced by <unk> are ", unknown_counter)

with open("vocab.txt", "w") as vocab_file:
    # Initialize index
    index = 0

    # Sort the vocabulary by frequency in descending order
    sorted_vocab = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    print("The total length of vocabulary is : ", len(sorted_vocab)+1)

    # Write the remaining vocabulary
    vocab_file.write(f"<unk>\t0\t{unknown_counter}\n")
    for i in range(index, len(sorted_vocab)):
        word, freq = sorted_vocab[i]
        vocab_file.write(f"{word}\t{i}\t{freq}\n")

    word_freq['<unk>'] = unknown_counter

prior_probs = defaultdict(dict)
cnt = defaultdict(int)
total_sentence = len(train_data)
for sex in train_data:
  tag = sex['labels'][0]
  cnt[tag] += 1

for tag in label_freq:
  prior_probs[tag] = cnt[tag] / total_sentence

# """Create HMM JSON file"""

tag_pair_counts = defaultdict(int)

for sentence in train_data:
    prev_tag = None
    labels = sentence['labels']
    for i in range(1,len(labels)):
        tag = labels[i]
        prev_tag = labels[i-1]
        tag_pair = (prev_tag, tag)
        tag_pair_counts[tag_pair] += 1

# Calculate probabilities
transition_probs_json = {}
for tag_pair, count in tag_pair_counts.items():
    tag1, tag2 = tag_pair
    transition_probs_json[str(tag_pair)] = count / label_freq[tag1]

emission_probs_json = {}
word_tag_counts = defaultdict(int)
for sentence in train_data:
    for word, tag in zip(sentence['sentence'], sentence['labels']):
        word = word.lower()
        if not word in word_freq:
            word = '<unk>'
        word_tag_counts[(word, tag)] += 1

for (word, tag), count in word_tag_counts.items():
    emission_probs_json[str((word, tag))] = count / label_freq[tag]


print("Total Transition parameters : ", len(transition_probs_json))
print("Total Emission parameters : ", len(emission_probs_json))

# Create model
model = {
    'initial': prior_probs,
    'transition': transition_probs_json,
    'emission': emission_probs_json
}

with open('hmm.json', 'w') as f:
    json.dump(model, f, indent=4)

tag_pair_counts = defaultdict(lambda: defaultdict(int))

for sentence in train_data:
  prev_tag = None
  labels = sentence['labels']

  for i in range(1, len(labels)):
    tag = labels[i]
    prev_tag = labels[i-1]
    tag_pair_counts[prev_tag][tag] += 1

transition_probs = defaultdict(dict)
for tag1 in label_freq:
    for tag2 in label_freq:
        # tag1, tag2 = tag_pair
        # print(tag1,tag2)
        transition_probs[tag1][tag2] = tag_pair_counts[tag1][tag2] / label_freq[tag1]


# word_tag_counts = defaultdict(dict)
# for sentence in train_data:
#   for word, tag in zip(sentence['sentence'], sentence['labels']):
#     word = word.lower()
#     if word not in word_freq:
#       word = '<unk>'
#     word_tag_counts[word][tag] += 1

word_tag_count = defaultdict(lambda: defaultdict(int))
emission_probs =  defaultdict(dict)
for item in train_data:
    sentence = item['sentence']
    # sentence = replaceUnknown(sentence,word_freq.keys())
    tags = item['labels']
    for i in range(len(sentence)):
        word = sentence[i].lower()
        if word not in word_freq:
            word = '<unk>'
        # print(word)
        word_tag_count[tags[i]][word] += 1


for tag in label_freq:
    for word in word_freq:
        emission_probs[tag][word] = word_tag_count[tag][word]/label_freq[tag]

# """Task 3: Greedy Decoding with HMM"""

def greedy_decode(sentence):
  best_labels = []
  sentence = [x.lower() for x in sentence]
  sentence = ['<unk>' if x not in word_freq else x for x in sentence]
  # print(len(sentence))

  prob = defaultdict(int)

  for label in prior_probs:
    prob[label] = prior_probs[label] * emission_probs[label][sentence[0]]
  best_label = max(prob,key=prob.get)

  best_labels.append(best_label)

  for i in range(1, len(sentence)):
    trans_prob = defaultdict(int)
    prev_label = best_labels[i-1]
    word = sentence[i]

    best_prob = 0
    best_label = None

    for label in transition_probs[prev_label]:
      trans_prob[label] = transition_probs[prev_label][label] * emission_probs[label][word]
    best_label = max(trans_prob,key=trans_prob.get)

    best_labels.append(best_label)

  return best_labels

greedy_pred_list = []

for j in dev_data:
  sen = j['sentence']
  # print(j['index'])
  res = greedy_decode(sen)
  greedy_pred_list.append(res)

arr = np.concatenate(np.array(greedy_pred_list))

greedy_accuracy = hmm_accuracy(dev_labels, arr)
print("Accuracy of Greedy Algorithm is : ", greedy_accuracy)

# """Creating Greedy.json for test data"""

greedy_pred_test_list = []

for j in test_data:
  sen = j['sentence']
  # print(j['index'])
  res_tes = greedy_decode(sen)
  greedy_pred_test_list.append(res_tes)

obj = create_json(test_data, greedy_pred_test_list)

with open('greedy.json', 'w') as f:
    json.dump(obj, f, indent=4)

# """Task 4: Viterbi Decoding with HMM"""

def viterbi_decode_algo(obs, states, emission_probs, transition_probs, prior_probs):
    S = len(states)
    O = len(obs)
    trellis = np.zeros((S,O))
    pointers = np.zeros((S,O))
    for s in range(S):
        trellis[s][0] = prior_probs[states[s]] * emission_probs[states[s]][obs[0]]
    for o in range(1,O):
        for s in range(S):
            probs = {}
            for k in range(S):
                probs[k]  = trellis[k][o-1] * transition_probs[states[k]][states[s]] * emission_probs[states[s]][obs[o]]
            max_k = max(probs, key=probs.get)
            trellis[s][o] = trellis[max_k][o-1] * transition_probs[states[max_k]][states[s]] * emission_probs[states[s]][obs[o]]
            pointers[s][o] = max_k

    best_path = []
    new_probs = {}
    for k in range(S):
        new_probs[k] = trellis[k][O-1]
    max_k = max(new_probs, key=new_probs.get)

    for o in range(O-1, -1, -1):
        best_path.insert(0, states[int(max_k)])
        max_k = pointers[int(max_k)][o]
    return best_path

viterbi_pred_list = []

states = list(label_freq.keys())
for ex in dev_data:
  # print(ex['index'])
  obs = ex['sentence']
  obs = [x.lower() for x in obs]
  obs = ['<unk>' if x not in word_freq else x for x in obs]
  # sentence = ex['sentence']
  rest = viterbi_decode_algo(obs, states, emission_probs, transition_probs, prior_probs)
  viterbi_pred_list.append(rest)

arr2 = np.concatenate(np.array(viterbi_pred_list))
viterbi_accuracy = hmm_accuracy(dev_labels, arr2)
print("Accuracy of Viterbi Algorithm is : ", viterbi_accuracy)


viterbi_pred_test_list = []

states = list(label_freq.keys())
for ex in test_data:
  # print(ex['index'])
  obs = ex['sentence']
  obs = [x.lower() for x in obs]
  obs = ['<unk>' if x not in word_freq else x for x in obs]
  # sentence = ex['sentence']
  rest = viterbi_decode_algo(obs, states, emission_probs, transition_probs, prior_probs)
  viterbi_pred_test_list.append(rest)

obj2 = create_json(test_data, viterbi_pred_test_list)

with open('viterbi.json', 'w') as f:
    json.dump(obj2, f, indent=4)